<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elizabeth Zagroba: Software Tester (Posts about automation)</title><link>https://elizabethzagroba.com/</link><description></description><atom:link href="https://elizabethzagroba.com/categories/automation.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>© 2020 &lt;a href="mailto:me@elizabethzagroba.com"&gt;Elizabeth Zagroba&lt;/a&gt; Mozilla Public License 2.0</copyright><lastBuildDate>Sun, 26 Jul 2020 18:27:17 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Test Automation Day 2018</title><link>https://elizabethzagroba.com/posts/2020/2020-07-13_test_automation_day_2018/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;I got a free ticket to Test Automation Day in 2018, just after I'd moved to Rotterdam. I was overwhelmed by the confluence of events: &lt;a href="https://twitter.com/techgirl1908"&gt;Angie Jones&lt;/a&gt; keynoting, &lt;a href="https://twitter.com/ard_kramer"&gt;Ard Kramer&lt;/a&gt; running the show, and meeting &lt;a href="https://twitter.com/amyjph"&gt;Amy Phillips&lt;/a&gt; in real life. )Neither of us were sure it was the first time we'd met because we'd been following each other on Twitter for so long!)&lt;/p&gt;
&lt;p&gt;My most shocking note from Angie's keynote is "clicker, no notes," because of course Angie had her talk down pat. In a talk that anticipated the current, urgent conversation in AI and machine learning, Angie recognized that we can't agree on what human ethics should look like. Figure out who you're advocating for, and tie the bugs back to that business value. You're not going to be able to define all the business requirements up front; expect the unexpected.&lt;/p&gt;
&lt;p&gt;Amy Phillips spoke about how tests in a DevOps environemnt allow you to get fast feedback. Like Agile, this style of working is not about minimizing the pain and struggle in developing software, but rather about bringing that pain forward. DevOps allows us to become aware of problems sooner, so we can act on them sooner. Running more tests is not necessarily better. Do not accept flaky tests. Free yourself from an overreliance on end-to-end tests and tests that cover non-critical paths to get your build time down. Rather than running tests on every commit, improve your monitoring. &lt;/p&gt;
&lt;p&gt;I've got some quotes from other talks and the panel that day: 
- "A tester is someone who believes things can be different." ~ Jerry Weinberg
- "The team was not mature enough to determine priorities."
- "Maintain the relationships you want to build."
- "Are you just doing it because you can?" (regarding UI automation)&lt;/p&gt;
&lt;p&gt;I can't tell everybody how welcome and in the right place I felt by being able to jump in this day on short notice. &lt;/p&gt;&lt;/div&gt;</description><category>automation</category><category>conference</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-07-13_test_automation_day_2018/</guid><pubDate>Sun, 12 Jul 2020 22:00:00 GMT</pubDate></item><item><title>This Too Shall Pass: Disposable Test Automation</title><link>https://elizabethzagroba.com/posts/2019/2019-03-29_this-too-shall-pass-disposable-test-automation/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p name="00f1" id="00f1" class="graf graf--p graf-after--h3"&gt;A few different times, we wrote some Python code to help us test our products. And then we threw the code out.&lt;/p&gt;

&lt;p name="ad39" id="ad39" class="graf graf--p graf-after--p"&gt;We had the infrastructure in place to add tests to our continuous integration pipeline in Jenkins. It would have been as simple as merging the branch of our code into master. But it had served its purpose already.&lt;/p&gt;

&lt;h5 name="7e5f" id="7e5f" class="graf graf--h4 graf-after--p"&gt;Example 1: web feature integrating with desktop software&lt;/h5&gt;

&lt;p name="1da2" id="1da2" class="graf graf--p graf-after--h4"&gt;Our team owner a web-based product. It had lots of features, but the two we were concerned with for this were: it created an account and a project. These would be used in a desktop product built by different teams at our company. For this story, a flag would be set when you created a project in our product to allow for something new in the desktop software.&lt;/p&gt;

&lt;p name="9663" id="9663" class="graf graf--p graf-after--p"&gt;Our testing stack was built and maintained by our team alone. It was set up to look at the web UI and APIs, but not the desktop software. We had APIs to create projects and change this new project flag. We didn’t have an automated way to see exactly what would happen in the desktop software under these different circumstances.&lt;/p&gt;

&lt;p name="c2c2" id="c2c2" class="graf graf--p graf-after--p"&gt;We wrote tests to query the APIs to see that the settings we set were coming back as expected. Those went into the pipeline. We also wrote some Python code to create projects in each of the five different states. Then, we manually went into the desktop software, used each of the projects we created, and looked at what happened in the desktop software. The information we discovered was enough to determine that the work for our team and the work for the desktop software teams was complete.&lt;/p&gt;

&lt;p name="8f6c" id="8f6c" class="graf graf--p graf-after--p"&gt;We did not add these tests to the pipeline. The branch got removed from the project without getting merged into master once the story was completed.&lt;/p&gt;

&lt;h5 name="5b5e" id="5b5e" class="graf graf--h4 graf-after--p"&gt;Example 2: crude performance test&lt;/h5&gt;

&lt;p name="a60d" id="a60d" class="graf graf--p graf-after--h4"&gt;We wanted to simulate the load placed on our product by a different internal app. Unfortunately the owner of the internal app was unavailable in the short period of time we had to complete this task. To do this, we took existing feature tests we had running on our staging environments, parallelize them, and run them on a clone of our production environment.&lt;/p&gt;

&lt;p name="4cc3" id="4cc3" class="graf graf--p graf-after--p"&gt;Our production clone was available during the few days we were doing this test. It would not be available thereafter, considering the time and money we would have to invest in maintaining it. Our other staging environments had a different enough capacity that running a performance test there would not be meaningful. Our production environment would give us the information we needed once we released this build because the internal app ran there. We maintained a branch for a few days while we were writing and using the performance test, but without an environment to run it on, we threw it out.&lt;/p&gt;

&lt;h5 name="eea5" id="eea5" class="graf graf--h4 graf-after--p"&gt;Example 3: audit trail Excel export&lt;/h5&gt;

&lt;p name="fc7f" id="fc7f" class="graf graf--p graf-after--h4"&gt;We added an audit trail to our profile information for GDPR compliance. Our system could display the information in the UI and export it to Excel. We added tests to our pipeline for the UI bit. The exporting to Excel bit we didn’t. We wrote a test that ended by providing us a username and password. Manually, we’d login, go to the page with the Excel export, and confirm that the data in the file matched the changes the test had made.&lt;/p&gt;

&lt;p name="c098" id="c098" class="graf graf--p graf-after--p"&gt;The Excel exporter wasn’t a piece of code our team maintained. If this test failed, it would have likely been in that functionality, since we also had a UI test for the data integrity. We weren’t changing anything about the Excel export. The audit trail report was an important enough feature that we knew we’d smoke test it manually with every release, so we didn’t add this code to the repository.&lt;/p&gt;

&lt;p&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2019/windows.png"&gt;&lt;/p&gt;
&lt;h5 name="a30f" id="a30f" class="graf graf--h4 graf-after--figure"&gt;What we asked ourselves when throwing out our automation&lt;/h5&gt;

&lt;ul class="postList"&gt;&lt;li name="a1f8" id="a1f8" class="graf graf--li graf-after--h4"&gt;What would we be asserting at the end of the test?&lt;/li&gt;&lt;li name="5de9" id="5de9" class="graf graf--li graf-after--li"&gt;If those asserts succeeded, would they give us false confidence that the feature was covered when we couldn’t account for the consequences?&lt;/li&gt;&lt;li name="f15d" id="f15d" class="graf graf--li graf-after--li"&gt;If these asserts failed, would that give us information about what to fix in our product?&lt;/li&gt;&lt;li name="90cc" id="90cc" class="graf graf--li graf-after--li"&gt;Would checking the code into the automation repository expose sensitive data about production?&lt;/li&gt;&lt;li name="2a56" id="2a56" class="graf graf--li graf-after--li graf--trailing"&gt;Would running these tests against our staging environments give us the information we needed?&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Originally published on &lt;a href="https://medium.com/@ezagroba/this-too-shall-pass-disposable-test-automation-6d0dadeff53"&gt;Medium&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>automation</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2019/2019-03-29_this-too-shall-pass-disposable-test-automation/</guid><pubDate>Thu, 28 Mar 2019 23:00:00 GMT</pubDate></item></channel></rss>