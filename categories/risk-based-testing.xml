<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elizabeth Zagroba: Organizational Anarchist (Posts about risk-based-testing)</title><link>https://elizabethzagroba.com/</link><description></description><atom:link href="https://elizabethzagroba.com/categories/risk-based-testing.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Â© 2022 &lt;a href="mailto:me@elizabethzagroba.com"&gt;Elizabeth Zagroba&lt;/a&gt; Mozilla Public License 2.0</copyright><lastBuildDate>Sun, 28 Aug 2022 14:47:48 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Not Every Detail Matters</title><link>https://elizabethzagroba.com/posts/2022/02_12_not_every_detail_matters/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;p&gt;I was looking at a user story for one of the teams I support. The story was about improving a very particular page. Our users do see it. But only for 5-10 minutes per week, if they've started their work early. We deploy this product weekly just before working hours. Deploying currently involves taking the whole product down. Customers can sign up for noticifications so they're reminded about this downtime window. &lt;/p&gt;
&lt;p&gt;The story was to improve the look of a page. People might see it and be confused if the stars aligned and:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They started work early.&lt;/li&gt;
&lt;li&gt;They hadn't signed up for the notification.&lt;/li&gt;
&lt;li&gt;They hadn't seen the web app with just a logo on it before.&lt;/li&gt;
&lt;li&gt;They didn't try it again in a few minutes. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So I asked the ticket writer, "This doesn't impact customers much (5-10 minutes per week). Is fixing this worth the effort?"&lt;/p&gt;
&lt;p&gt;They wrote back "I believe in 'Every detail matters.' This particular detail should take very little effort to realize, so my answer on this question is Yes."&lt;/p&gt;
&lt;p&gt;It's possible they're right to pick this ticket up. It was a small enough effort that we might as well do it. If they're wrong, they're the one feeling the pain of explaining the ticket to the team, verifying the fix, deciding what to put in the release notes, etc. It's a safe-to-fail experiment for me as a quality coach. &lt;/p&gt;
&lt;p&gt;But I didn't have the same mindset. I don't believe that we should fix everything we find in our app that violates my expectations. I don't think it's possible to identify one correct set of expectations and priorities that our users will share. I don't think the things that we've already fixed will stay fixed. I don't think it's possible to cover every issue with an automated test. &lt;/p&gt;
&lt;p&gt;I think we need to let go. We need to decide what's important, and focus on that. The details of the downtime page -- the new design, and the time the team spent updating it, and the effort I'd spend having the conversation about it -- none of them mattered too much to me. We need to notice details, and also know when to turn our brains off to being bothered by them. We need to think about the risks of our tests could uncover; our goal isn't 100% test coverage. In short:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not every detail matters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are limited by our attention, energy, health, meetings on our schedule, time left on this earth. Software is complex enough that it's very unlikely we'll be able to solve every issue we find. The more time we spend solving the unimportant ones, the less time we have left to look for the important ones. Or decide what is important. Or understand our users better to be able to more effectively evaluate the relative importance of such issues. &lt;/p&gt;
&lt;p&gt;Jerry Weinberg cheekily noted the impossibility of this endeavor in his book accurately titled &lt;a href="https://app.thestorygraph.com/books/8ba29269-1843-4ac1-be0c-226752b17937"&gt;&lt;em&gt;Perfect Software and Other Illusions About Testing&lt;/em&gt;&lt;/a&gt;. The Black Box Software Testing Course on Test Design emphasized the need for testers to balance risk vs. coverage. Its focus on &lt;a href="https://www.developsense.com/blog/2010/05/why-we-do-scenario-testing/"&gt;scenario testing&lt;/a&gt; insisted we tie our testing to a user's journey through the software that was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coherent&lt;/li&gt;
&lt;li&gt;credible&lt;/li&gt;
&lt;li&gt;motivating&lt;/li&gt;
&lt;li&gt;complex&lt;/li&gt;
&lt;li&gt;easy to evaluate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I know this is the right approach. It will leave time to build new features and learn new skills. It's what will make it possible for us to feel fulfilled and motivated in our work. &lt;/p&gt;
&lt;p&gt;Now I just need to figure out how to scale this mindset. &lt;/p&gt;</description><category>coaching</category><category>mindset</category><category>risk-based-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2022/02_12_not_every_detail_matters/</guid><pubDate>Fri, 11 Feb 2022 23:00:00 GMT</pubDate></item><item><title>Map Out Your Stakeholders</title><link>https://elizabethzagroba.com/posts/2021/map_out_your_stakeholders/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;p&gt;Test reporting is part of a feedback loop. It's the beginning of a conversation, not the end. Knowing who you're having that conversation with allows you to provide those individuals better information for their context. &lt;/p&gt;
&lt;p&gt;If you find a big nasty bug, you might report it differently if your audience is a developer on your team who you work with everyday, a developer on another team who you haven't met, or the Head of Product looking to give an important demo. Reporting on the breath, depth, focus, and impediments to your testing can help your audience guide your upcoming testing. &lt;/p&gt;
&lt;p&gt;Joep Schuurkes and I had an activity as part of workshop on test reporting at TestBash Manchester 2019. I believe he articulated the key idea: if your test reporting depends on your audience, you have to know who your audience is. We had participants map out (with paper and markers) who the stakeholders were for their testing. Some people drew org charts, other drew mind maps.&lt;/p&gt;
&lt;p&gt;In the test reporting workshop I held yesterday, we used a Miro board to map out our stakeholders. As examples, I made an overview of how I was thinking about my recent team. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2021/stakeholder-mind-map.png"&gt;&lt;/p&gt;
&lt;p&gt;And a version of Dan Ashby's Layers of Influence model, the "shallot" of influence, if you will. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2021/stakeholder-shallot.png"&gt;&lt;/p&gt;
&lt;p&gt;While these are stated with people's roles, doing this for yourself using people's actual names (or names + roles) will help you think about who they are and what they listening for. &lt;/p&gt;
&lt;p&gt;Identifying the audience for your test report allows you to tailor it to the risks they care about. If you're not sure how to tailor the report, present them with something and find out if that's what they want. Even better, share with them that you're trying to figure out how to make your work most effective for them. &lt;/p&gt;
&lt;p&gt;More things to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Other posts on &lt;a href="https://elizabethzagroba.com/categories/reporting/"&gt;test reporting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://club.ministryoftesting.com/t/ask-me-anything-test-reporting/46827"&gt;Questions left over from an Ask Me Anything on test reporting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Having the conversation about the conversation, or &lt;a href="https://elizabethzagroba.com/posts/2021/delivering_information_vs_delivering_meta_information/"&gt;delivering meta-information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><category>reporting</category><category>risk-based-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2021/map_out_your_stakeholders/</guid><pubDate>Thu, 28 Oct 2021 22:00:00 GMT</pubDate></item><item><title>Unblocking Your Test Strategy</title><link>https://elizabethzagroba.com/posts/2021/unblocking_your_test_strategy/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;figure&gt;&lt;img src="https://elizabethzagroba.com/images/posts/2021/lightbulb.jpeg"&gt;&lt;/figure&gt; &lt;p&gt;In my new role as Quality Lead for my department, I get to figure out how to infuse everybody's work with "quality", and also figure out what that means exactly. &lt;/p&gt;
&lt;p&gt;One of my colleagues made it easy for me on my second day by coming with a relatively concrete problem: they wanted an acceptance environment for their team. Their team (henceforth: Eager Team) integrated with chronically overloaded and busy team (henceforth: Busy Team), so they wanted an environment where they could test their stuff together before it went into production. They wanted me to help set that up. &lt;/p&gt;
&lt;p&gt;I started my conversation with Eager Team Lead by taking one step back: why did they want this environment? They'd proposed a solution, but I wanted to spend at least a few minutes digging into the problem space with them to hear more about why they wanted this.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Come up with dream scenario&lt;/h4&gt;
&lt;p&gt;I asked Eager Team Lead what their dream setup would be for their test automation, and why that was the dream.&lt;/p&gt;
&lt;p&gt;Eager Team and Busy Team already had a test environment hooked up to one another. But they both threw whatever they were in the middle of on that environment. Eager Team couldn't count on a stable, usable version of Busy Team's software, and vice versa. Eager Team wanted a place to see what would happen against the production version of Busy Team's code. They wanted to automate all the things they could, and have a place to run that automation. &lt;/p&gt;
&lt;h4&gt;Identify (and confirm they are indeed) constraints&lt;/h4&gt;
&lt;p&gt;Unfortunately Busy Team was busy. They wouldn't be able to make setting up an environment for Eager Team a priority in the next few months. I had that impression, and so did Eager Team Lead. They were, after all, Busy Team. But I wanted to make sure that the busyness of Busy Team was a constraint. I took on the action point to follow up with Boss Person about how we could both (1) check that Busy Team was indeed too busy, and (2) how to get this request on Busy Team's long list for the future.&lt;/p&gt;
&lt;p&gt;I also dispelled one of assumptions underlying Eager Team Lead's dream setup: it was important to test everything, in an automated way, in the ideal environment, or else testing wouldn't be valuable. I explained that it's &lt;a href="https://app.thestorygraph.com/books/8ba29269-1843-4ac1-be0c-226752b17937"&gt;impossible to test everything&lt;/a&gt;. Testing in an automated way would be more likely to reveal known unknowns than the unknown unknowns their team was interested in. And that it wasn't all-or-nothing - every little bit would help.&lt;/p&gt;
&lt;h4&gt;Choose achieveable pieces within constraints&lt;/h4&gt;
&lt;p&gt;Rather than killing the dream, I identified a valuable first step in the direction of the dream. Eager Team would write down, in English to start, 3-5 things that they want to test using both their software and Busy Team's. They'd show those to their product owner to make sure they were things customers cared about. From there, we could look at whether to build automation, and if so, where to run it. There was that test environment already. We had production, could we use feature flags? Could we keep the data only visible to our employees internally? &lt;/p&gt;
&lt;p&gt;I knew I'd hit a nerve when Eager Team Lead said "Oh, we can just start iterating over this!" Because of course, the software itself is not the only thing you can build in an iterative way. Your test automation can also mitigate risk, confirm assumptions, and provide value along the way. &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;So how'd it go? I confirmed Busy Team's busyness, and got more details on how and when to add this request to their list. I'm following up with Eager Team next week to see where they are in identifying valuable scenarios, or if I should jump in there too. &lt;/p&gt;
&lt;p&gt;But wow, what a feeling to be able to lift the weight of "I need a thing I don't know how to build and don't think I can ever get" off someone's shoulders and replace it with "I know what to do next and it's achievable." &lt;/p&gt;
&lt;p&gt;Stay tuned for more quality leading to come.&lt;/p&gt;</description><category>automation</category><category>coaching</category><category>risk-based-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2021/unblocking_your_test_strategy/</guid><pubDate>Thu, 21 Oct 2021 22:00:00 GMT</pubDate></item><item><title>Complete the Main Quest First</title><link>https://elizabethzagroba.com/posts/2021/complete_the_main_quest_first/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;p&gt;Recently, I made an outline for a tester (who was still onboarding) for what kinds of things to test on a new API endpoint we added. They explored, wrote a bunch of automated tests to capture their work, and came back with a list of interesting and good catches in the error responses. My first question in our debrief was: did you try a successful response? They hadn't. I sent them back to tackle that too. &lt;/p&gt;
&lt;p&gt;Because a successful response is the first thing our product owner is going to ask about. That's what we'd want to show off at the review meeting internally to demonstrate the new API endpoint. That's the first the customer is going to try. They're going to copy the request from our OpenAPI specification, paste it in Postman (or the tool of their choice, but our customers so far have been using Postman), and see if their credentials will get them the response that matches the specification. These stakeholders share a common concern, and that's the risk we should be migitating with testing. First. &lt;/p&gt;
&lt;h3&gt;Complete the main quest first.&lt;/h3&gt;
&lt;p&gt;Complete the main quest first. Come back to the side quests. &lt;/p&gt;
&lt;p&gt;A customer had asked for this API endpoint to be added. If we'd tested the happy path first, we would have had the option of releasing the API for the customer to use. The risk of discovering a successful request wouldn't yield a successful response was relatively low in this case, since our developers tend to try one happy path themselves.&lt;/p&gt;
&lt;p&gt;But what if the main quest had required a lot of setup, explanations to build knowledge and context for the onboarding tester, or yielded an issue? I'd done a risk-based analysis of what all to complete as part of our definition of done for this story. But I hadn't shared my approach to completing the main quest first, so the tester did what testers do, and went on a hunt to find weird stuff. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2021/iceland.jpg"&gt;&lt;/p&gt;
&lt;h3&gt;Note down and follow-up on werid stuff; do not get distracted by it&lt;/h3&gt;
&lt;p&gt;Software will break in all sorts of ways. The more time and curiosity you have to dig into it, the more you'll discover. But are those the most important things? &lt;/p&gt;
&lt;p&gt;In this API, the tester discovered that if you paste 10,000 characters into a field that's meant for a UUID, you get a 400 response. But did they try a regular old UUID first? What if they get a 400 response no matter what they put in that field, because the field name in the specification doesn't match what's in the code? Is trying 10,000 characters the first and biggest risk they have to face when presenting this API to a customer?&lt;/p&gt;
&lt;p&gt;I'm not saying don't try 10,000 characters. &lt;a href="https://twitter.com/ezagroba/status/1234822805709053953"&gt;I love that shit&lt;/a&gt;. But decide if it's a risk you care about first. &lt;a href="https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/"&gt;If you don't care about the outcome, don't test it&lt;/a&gt;. Don't make busy-work for yourself just to fill the time. &lt;/p&gt;
&lt;h3&gt;Make side quests a concious choice&lt;/h3&gt;
&lt;p&gt;Before you start throwing 10,000 characters at your API, talk to your team. Your developer can probably tell you if they never built something to deal with that situation. Your product owner can tell you they'd rather have it to the customer sooner. Your data analyst can tell you if there's already longer stuff than that in the database, or if you should be trying Japanese instead. &lt;/p&gt;
&lt;p&gt;Make side quests a deliberate choice. Share them to increase their value or figure out who on the team is best-suited to execute them. &lt;/p&gt;
&lt;h3&gt;Recognize when the quest is a journey, not a destination&lt;/h3&gt;
&lt;p&gt;Throwing 10,000 characters at an API may be a way to start a discussion about the speed at which responses are returned. It might be a way of showing &lt;a href="http://coffeeipsum.com/"&gt;your favorite random text generator&lt;/a&gt; to your fellow tester. It might be an exercise at an ensemble testing session, where everyone can practice pausing before executing an idea to describe the expected behavior first. &lt;/p&gt;
&lt;p&gt;Quests can be valuable in ways that are not directly related to the finishing the quest. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: I got asked recently if I use the word charter much with non-testers. I don't. Try reading this again but replacing every mention of "quest" with "charter".&lt;/em&gt;&lt;/p&gt;</description><category>charters</category><category>critical-thinking</category><category>exploratory-testing</category><category>mindset</category><category>reporting</category><category>risk-based-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2021/complete_the_main_quest_first/</guid><pubDate>Sat, 03 Jul 2021 22:00:00 GMT</pubDate></item><item><title>Questions from Exploratory Week on Writing Exploratory Testing Charters</title><link>https://elizabethzagroba.com/posts/2021/questions_from_exploratory_week_on_writing_exploratory_testing_charters/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;figure&gt;&lt;img src="https://elizabethzagroba.com/images/posts/2021/MoT.jpg"&gt;&lt;/figure&gt; &lt;p&gt;The Ministry of Testing hosted a week all about exploratory testing. I had the honor and privilege to help shepherd a small group of testers on the path of writing charters for their exploration. The most interesting part for me is where people had questions. It helps me figure out what sunk in, what could use more explanation, and helps me know that I've answered at least one person's burning question. Here are some of the ones I remember from the live Q&amp;amp;A at the end:&lt;/p&gt;
&lt;h4&gt;Q. Do you use the word charter?&lt;/h4&gt;
&lt;p&gt;A. Basically no. I've only heard testers who've specifically dug into this topic use the word charter. Almost all of the people I collaborate with on a daily basis (developers, product owner, UX, managers, other testers) do not have this as part of their experience. Most of my colleagues are not working in their first language. As a native speaker, I need to have more than one word to describe any particular phenomenon in case the first one doesn't resonate, or isn't understandable in my accent. (Everyone has an accent.) I've called charters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;questions&lt;/li&gt;
&lt;li&gt;missions&lt;/li&gt;
&lt;li&gt;paths&lt;/li&gt;
&lt;li&gt;plans&lt;/li&gt;
&lt;li&gt;goals&lt;/li&gt;
&lt;li&gt;investigations&lt;/li&gt;
&lt;li&gt;journeys&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's less important to use the word charter than it is to get across the intent: you're going on an exploration, in a particular direction, with specific set of tools, and you hope to come away with more information on this topic than when you set out. Sharing your charters helps you get feedback about where to look more deeply, more broadly, and &lt;a href="https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/"&gt;where not to look at all&lt;/a&gt;. &lt;/p&gt;
&lt;h4&gt;Q. Where do you bring charters up?&lt;/h4&gt;
&lt;p&gt;A. Where &lt;em&gt;don't&lt;/em&gt; I bring charters up? It bleeds into conversations I have about my work. &lt;a href="https://youtu.be/SM57HMJpkZc?t=974"&gt;Sharing my work and getting feedback about it is what ensures I'm providing valuable work for my team in the right direction&lt;/a&gt;. I tend to discover points of interest for my developers once or twice a day when development is starting, and more often when testing is at its peak, which often escalates to pairing. Here are some other moments in time where I share charters: &lt;/p&gt;
&lt;h5&gt;Standup&lt;/h5&gt;
&lt;p&gt;It's how I explain what I tested yesterday, what pieces I might have time for today, and what directions I haven't or won't have time for before we want to release the story. Sharing where I'm looking prevents me from being the one gatekeeper on quality for our product. "I've successfully called the API as an admin user and a regular user. Today I'm going to dig into what happens with the non-required fields." will solicit a completely different type of feedback than "I have an hour or two left on this story."&lt;/p&gt;
&lt;h5&gt;Refinement&lt;/h5&gt;
&lt;p&gt;Any clues I can give my team about what I'll be looking into, what kind of test data I might set up, and what tools I'll be using to test a particular feature will help them figure out the whole scope of the story. "I'm going to try names at the character limit to see how they wrap on the page." helps us all figure out that we need to talk about our expectations for a character limit, we need to talk to UX about what should happen when you try to input something too long, I need to test what happens on the API side for the same field, and we might need a frontend dev to help us with the wrapping or truncation depending on what UX decides.&lt;/p&gt;
&lt;h5&gt;Testing starts executing&lt;/h5&gt;
&lt;p&gt;This is the point in time where there's enough built that I can add test execution to the setup and planning I've already been doing on a story. It might be that the API spec is published, it might be that the application has one happy path built. The developers are still going, but there's somewhere for me to start. Depending on the size and complexity of the story, I'll reflect for myself, or share my ideas with someone else on the team. If it involves an integration with another team, I'd reach out to them too. &lt;/p&gt;
&lt;h4&gt;Q. Isn't that just a test case?&lt;/h4&gt;
&lt;p&gt;A. After almost two hours of technical difficulties and explaining things, I have to say I did not write the most elegant charter as an example during the workshop. You got me! I'm glad that this workshop participant has a good feel for what is too specific or too broad. I find this so hard to explain because so much of that depends on the context. &lt;/p&gt;
&lt;p&gt;But it wasn't terribly important to me to get the level of detail correct. Charters are a place to reflect on your testing and spark conversation. This charter did exactly that. &lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;You can find other questions there weren't time for during the workshop on the &lt;a href="https://club.ministryoftesting.com/t/wander-with-a-purpose-writing-charters-for-your-exploratory-test-sessions/49746"&gt;Ministry of Testing club&lt;/a&gt;. The &lt;a href="http://ezagroba.github.io/charters"&gt;slides from the workshop are on github&lt;/a&gt;. &lt;/p&gt;</description><category>charters</category><category>exploratory-testing</category><category>risk-based-testing</category><category>teaching</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2021/questions_from_exploratory_week_on_writing_exploratory_testing_charters/</guid><pubDate>Fri, 30 Apr 2021 22:00:00 GMT</pubDate></item><item><title>Finding relevant search results</title><link>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;p&gt;In his crafting time, one of our developers decided to fine-tune our search results. He added relevancy scoring to give weights to different text fields. It was my job to determine if the right results were turning up at the top of the list of search results. So I had to ponder: what made a search result relevant? &lt;/p&gt;
&lt;p&gt;First, I realized that feedback from our users is the best way to answer this question. Anything I could do to get this feature out into production, where we'd get real data about what people searched for, would be more valuable that brainstorming test cases for this feature. I set myself a timebox of two and a half hours, the rest of the afternoon on a day in a week filled with competing priorities. We'd agreed as a team ahead of time that I could determine the testing approach, and our product owner would decide what was or wasn't worth fixing before this feature went out. &lt;/p&gt;
&lt;p&gt;I saved the first 45 minutes of my timebox to research what people had to say about search relevancy. Surely I was not the first person contemplating this problem. Over on the Ministry of Testing Club forum, I found what seemed to be a promising title of a post. &lt;a href="https://club.ministryoftesting.com/t/how-would-you-test-a-search-api/28027"&gt;It turned out a past Elizabeth from a year ago wrote it, and nobody had answered it satisfactorily in the intervening time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After Duck-Duck-Go-ing some actual answers, I found a couple of resources from websites I've trusted and found fruitful in the past: &lt;a href="https://alistapart.com/article/testing-search-for-relevancy-and-precision/"&gt;A List Apart&lt;/a&gt; and &lt;a href="https://www.philosophe.com/archived_content/search_topics/search_tests.html"&gt;philosophe.com&lt;/a&gt;. A List Apart suggested honing in on a specific intent, searching for the queries users seek most frequently, and seeing how far those items fall from the top in the search results. The philosophe guidance about testing search gave me something deeper to consider: users shouldn't have to ponder the reasoning behind the search results. That was enough for me to develop some test cases. &lt;/p&gt;
&lt;p&gt;As I searched and adjusted the weights of various fields, plenty of normal things happened: setting the relevancy values to zero meant the result wasn't listed, multiple instances of the same word counted more than a single instance, and giving fields stronger weights caused their results to move up in the rankings. But as a bug magnet, I uncovered things that were both interesting to discover and outside the original scope of the story. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/log-of-zero.png" style="display:block; margin-left: auto; margin-right: auto;" title="Log of zero is negative infinity"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Log of zero is negative infinity&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Bugs&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I opted to edit data that was already in our test environment rather than setting up completely new data. In doing so, I discovered a couple description fields that were missing an indexing update in the search results when edited through the UI. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I tried to use the default values for everything to see "normal" results. One field was going to add a value to the relevancy rating, so zero seemed like it should be the default option. Unfortunately a couple of the options for the weighting feature transformed the value using the &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;ln&lt;/code&gt; (natural log) functions, which are undefined at zero. All my search results disappeared. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I looked at the data that was already in the database, and used a search term that showed up a lot already. It turned up nothing. I searched for part of the word. That turned up all the results I was expecting. I realized the search term was indexed separately because of the characters we used to break the word apart. Imagine having a bunch of &lt;code&gt;sun-bleached&lt;/code&gt; items, but you can only find them if you search for &lt;code&gt;sun&lt;/code&gt; or &lt;code&gt;bleached&lt;/code&gt;, not &lt;code&gt;sun-bleached&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bug 1 the developer agreed was a bug, and we fixed as part of the story. Bug 2 was "working as expected" from a developer point-of-view, but seemed a little weird to the product owner. We meant to look into as part of the story and decide if we should eliminate the log functions as options entirely, but other priorities came crashing down upon before we could. It's out on production to the handful of internal users with access to the relevancy tuning. Bug 3 we added to the backlog, and I hope someday a swarm of user complaints make it a priority for us. &lt;/p&gt;
&lt;h4&gt;Morals&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Users know best.&lt;/li&gt;
&lt;li&gt;Someone else on the internet has had your problem before. &lt;/li&gt;
&lt;li&gt;Report information that matters.&lt;/li&gt;
&lt;li&gt;When the risk is low, let it go. &lt;/li&gt;
&lt;/ol&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>search</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</guid><pubDate>Sun, 20 Sep 2020 22:00:00 GMT</pubDate></item><item><title>If a test falls in a forest...</title><link>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;p&gt;The saying goes "If a tree falls in a forest and no one is around to hear it, does it make a sound?" I have similar question that shapes the way I think about software testing: If a test is performed but no one takes action on the results, should I have performed it? I think not. &lt;/p&gt;
&lt;p&gt;If the answer to "Who cares?" is "No one," don't perform that test. If you're not going to take action on the results of your testing in the coming hours, days, or weeks, don't perform that test. The world around you will change in the meantime, and the old results will not be as valuable.&lt;/p&gt;
&lt;p&gt;One of the &lt;a href="https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"&gt;12 Agile Principles&lt;/a&gt; is simplicity, or maximizing the work not done. Testing on an agile team provides information to help decide what work should picked up in the coming iteration(s). But without meaningful collaboration or feedback, testing is a pile of work for no reason. Work is not meant to produce waste. Save your time and your sanity by thoughtfully analyzing what should not be done, and coming to an agreement with your team about it.&lt;/p&gt;
&lt;p&gt;My team gets scared about the quality of our product and skeptical about how I'm using my time when I describe what I'm not testing, or which automated tests I'm not going to run. "But isn't testing your job?" says the look on their faces. "But then what are you going to do?" is what they manage to say. Rather than capitulating for appearances, to just "look busy," I take this as a challenge to make my exploratory testing and other work I'm doing for the team more visible. &lt;/p&gt;
&lt;h4&gt;Risk-based testing&lt;/h4&gt;
&lt;p&gt;In her &lt;a href="https://www.ministryoftesting.com/dojo/series/testbash-home/lessons/reverse-engineer-your-way-to-adopting-a-risk-based-testing-approach-nishi-grover-garg"&gt;TestBash Home talk&lt;/a&gt;, &lt;a href="https://twitter.com/testwithnishi"&gt;Nishi Grover Garg&lt;/a&gt; asked us to think about estimating impact and likelihood (with possible home intruders as an example). I'd have trouble pinning down our no-estimates team on concrete numbers for undesireable software behavior. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/nishi-impact-likelihood.png" style="display:block; margin-left: auto; margin-right: auto;" title="Slide from Nishi's TestBash Home talk"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Slide from Nishi's TestBash Home talk&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But it does reflect the conversations &lt;a href="http://quality-intelligence.com/documents/DesignBehindthePlan.pdf"&gt;Fiona Charles's test strategy workshop&lt;/a&gt; encouraged me to spark on my team. We do talk about "Yes, this would be a problem, but customers can use this work-around." Or "Yes, we could dive in and investigate whether than could ever happen, but is that more important than picking up the next story?" Being able to identify risks and have thoughtful conversations about their threat to stakeholders allows us to make informed decisions about how we should be spending our time. In testing, we don't always want the most information, we want to discover the best information about the product as efficiently as we can. &lt;/p&gt;
&lt;h4&gt;Examples from my current project&lt;/h4&gt;
&lt;h5&gt;Cross-browser testing&lt;/h5&gt;
&lt;p&gt;We were preparing our web application for a big marketing presentation. The presenter had Firefox as the default browser on their PC. We had a script of the actions they'd perform on stage, and which pages the audience would see. I happened to find bugs on pages we weren't showing, or in the way the scroll bars behaved in Chrome rather than Firefox on my Mac. &lt;/p&gt;
&lt;p&gt;I did not add these issues as bugs in our tracking system, or dig into them further. I knew that they did not pose a risk for the presentation, and a new design would be coming along before customers would potentially use those pages in Chrome on a Mac.s &lt;/p&gt;
&lt;h5&gt;The pipeline&lt;/h5&gt;
&lt;p&gt;We have a pipeline. It runs the tests we've automated at the API and the browser levels against the build in our test environment. I hoped it would inspire the team to think about what the next step could be: getting the tests to run against before merging into our main line, setting up an environment where we're not dependent on the (shared) test environment, looking at the results to see where our application or tests need to change. &lt;/p&gt;
&lt;p&gt;But we don't look at the results. We don't have alerts, we don't open the page during standup, we don't use them as a reference when we're debugging, we don't have a habit of looking at the results. If we do happen to look at the results, we don't take action on it. Building the stability of our feedback loop is not seen as high-priority a task as building new features. &lt;/p&gt;
&lt;p&gt;We don't need to run this pipeline. It's using up AWS resources. Looking at the long line of red X's on the results page only provides alert fatigue. We would be better served by not running these tests. &lt;/p&gt;
&lt;h5&gt;Minimum viable deadline&lt;/h5&gt;
&lt;p&gt;We promised to deliver a feature to a dependent team by a sadline. (A sadline is a deadline without consequences.) In the week before the sadline, three stories were left. On the first story, I found a mistake the developer declared "superficial" when he was lamenting our lack of &lt;a href="https://katrinatester.blogspot.com/2016/12/the-testing-pendulum-finding-balance-in.html"&gt;deep testing&lt;/a&gt;. He decided to review the automated tests I'd written for the second story. He found a couple of use-cases that would require a very particular set of circumstances to occur. I wanted to encourage the behavior of reviewing the tests and thinking about what they're doing more deeply, so I spent the last hour and a half before a holiday weekend automating these two cases. &lt;/p&gt;
&lt;p&gt;I'd drafted some basic automated tests for the third story, but the last feature went relatively unexplored. I should have used my scant time to test the third story more thoroughly instead. The complicated tests for the second story could have waited until next week. While we would be curious about the results, it would not have stopped our delivery of the feature. I should not have written them. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/far-side-tree-falling.jpg" style="display:block; margin-left: auto; margin-right: auto;" title="Far Side cartoon"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Far Side cartoon&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;You may be scared to say no to testing things that don't matter, where the performance will not reveal any risks or cause any follow-up actions to take place. It may be tempting to spend a bunch of time testing all the things you can think of, and only reporting on the tests that yield meaningful results. &lt;/p&gt;
&lt;p&gt;But life is not about keeping busy. Make your time at work meaningful by executing meaningful work and declining to do things that aren't important right now.&lt;/p&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>testbash</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</guid><pubDate>Sat, 23 May 2020 22:00:00 GMT</pubDate></item></channel></rss>