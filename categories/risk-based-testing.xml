<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elizabeth Zagroba: Software Tester (Posts about risk-based-testing)</title><link>https://elizabethzagroba.com/</link><description></description><atom:link href="https://elizabethzagroba.com/categories/risk-based-testing.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>¬© 2020 &lt;a href="mailto:me@elizabethzagroba.com"&gt;Elizabeth Zagroba&lt;/a&gt; Mozilla Public License 2.0</copyright><lastBuildDate>Thu, 24 Sep 2020 19:21:23 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Finding relevant search results</title><link>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;In his crafting time, one of our developers decided to fine-tune our search results. He added relevancy scoring to give weights to different text fields. It was my job to determine if the right results were turning up at the top of the list of search results. So I had to ponder: what made a search result relevant? &lt;/p&gt;
&lt;p&gt;First, I realized that feedback from our users is the best way to answer this question. Anything I could do to get this feature out into production, where we'd get real data about what people searched for, would be more valuable that brainstorming test cases for this feature. I set myself a timebox of two and a half hours, the rest of the afternoon on a day in a week filled with competing priorities. We'd agreed as a team ahead of time that I could determine the testing approach, and our product owner would decide what was or wasn't worth fixing before this feature went out. &lt;/p&gt;
&lt;p&gt;I saved the first 45 minutes of my timebox to research what people had to say about search relevancy. Surely I was not the first person contemplating this problem. Over on the Ministry of Testing Club forum, I found what seemed to be a promising title of a post. &lt;a href="https://club.ministryoftesting.com/t/how-would-you-test-a-search-api/28027"&gt;It turned out a past Elizabeth from a year ago wrote it, and nobody had answered it satisfactorily in the intervening time&lt;/a&gt;. ü§¶‚Äç‚ôÄÔ∏è&lt;/p&gt;
&lt;p&gt;After Duck-Duck-Go-ing some actual answers, I found a couple of resources from websites I've trusted and found fruitful in the past: &lt;a href="https://alistapart.com/article/testing-search-for-relevancy-and-precision/"&gt;A List Apart&lt;/a&gt; and &lt;a href="https://www.philosophe.com/archived_content/search_topics/search_tests.html"&gt;philosophe.com&lt;/a&gt;. A List Apart suggested honing in on a specific intent, searching for the queries users seek most frequently, and seeing how far those items fall from the top in the search results. The philosophe guidance about testing search gave me something deeper to consider: users shouldn't have to ponder the reasoning behind the search results. That was enough for me to develop some test cases. &lt;/p&gt;
&lt;p&gt;As I searched and adjusted the weights of various fields, plenty of normal things happened: setting the relevancy values to zero meant the result wasn't listed, multiple instances of the same word counted more than a single instance, and giving fields stronger weights caused their results to move up in the rankings. But as a bug magnet, I uncovered things that were both interesting to discover and outside the original scope of the story. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/log-of-zero.png" style="display:block; margin-left: auto; margin-right: auto;" title="Log of zero is negative infinity"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Log of zero is negative infinity&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Bugs&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I opted to edit data that was already in our test environment rather than setting up completely new data. In doing so, I discovered a couple description fields that were missing an indexing update in the search results when edited through the UI. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I tried to use the default values for everything to see "normal" results. One field was going to add a value to the relevancy rating, so zero seemed like it should be the default option. Unfortunately a couple of the options for the weighting feature transformed the value using the &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;ln&lt;/code&gt; (natural log) functions, which are undefined at zero. All my search results disappeared. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I looked at the data that was already in the database, and used a search term that showed up a lot already. It turned up nothing. I searched for part of the word. That turned up all the results I was expecting. I realized the search term was indexed separately because of the characters we used to break the word apart. Imagine having a bunch of &lt;code&gt;sun-bleached&lt;/code&gt; items, but you can only find them if you search for &lt;code&gt;sun&lt;/code&gt; or &lt;code&gt;bleached&lt;/code&gt;, not &lt;code&gt;sun-bleached&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bug 1 the developer agreed was a bug, and we fixed as part of the story. Bug 2 was "working as expected" from a developer point-of-view, but seemed a little weird to the product owner. We meant to look into as part of the story and decide if we should eliminate the log functions as options entirely, but other priorities came crashing down upon before we could. It's out on production to the handful of internal users with access to the relevancy tuning. Bug 3 we added to the backlog, and I hope someday a swarm of user complaints make it a priority for us. &lt;/p&gt;
&lt;h4&gt;Morals&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Users know best.&lt;/li&gt;
&lt;li&gt;Someone else on the internet has had your problem before. &lt;/li&gt;
&lt;li&gt;Report information that matters.&lt;/li&gt;
&lt;li&gt;When the risk is low, let it go. &lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>search</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</guid><pubDate>Sun, 20 Sep 2020 22:00:00 GMT</pubDate></item><item><title>If a test falls in a forest...</title><link>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;The saying goes "If a tree falls in a forest and no one is around to hear it, does it make a sound?" I have similar question that shapes the way I think about software testing: If a test is performed but no one takes action on the results, should I have performed it? I think not. &lt;/p&gt;
&lt;p&gt;If the answer to "Who cares?" is "No one," don't perform that test. If you're not going to take action on the results of your testing in the coming hours, days, or weeks, don't perform that test. The world around you will change in the meantime, and the old results will not be as valuable.&lt;/p&gt;
&lt;p&gt;One of the &lt;a href="https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"&gt;12 Agile Principles&lt;/a&gt; is simplicity, or maximizing the work not done. Testing on an agile team provides information to help decide what work should picked up in the coming iteration(s). But without meaningful collaboration or feedback, testing is a pile of work for no reason. Work is not meant to produce waste. Save your time and your sanity by thoughtfully analyzing what should not be done, and coming to an agreement with your team about it.&lt;/p&gt;
&lt;p&gt;My team gets scared about the quality of our product and skeptical about how I'm using my time when I describe what I'm not testing, or which automated tests I'm not going to run. "But isn't testing your job?" says the look on their faces. "But then what are you going to do?" is what they manage to say. Rather than capitulating for appearances, to just "look busy," I take this as a challenge to make my exploratory testing and other work I'm doing for the team more visible. &lt;/p&gt;
&lt;h4&gt;Risk-based testing&lt;/h4&gt;
&lt;p&gt;In her &lt;a href="https://www.ministryoftesting.com/dojo/series/testbash-home/lessons/reverse-engineer-your-way-to-adopting-a-risk-based-testing-approach-nishi-grover-garg"&gt;TestBash Home talk&lt;/a&gt;, &lt;a href="https://twitter.com/testwithnishi"&gt;Nishi Grover Garg&lt;/a&gt; asked us to think about estimating impact and likelihood (with possible home intruders as an example). I'd have trouble pinning down our no-estimates team on concrete numbers for undesireable software behavior. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/nishi-impact-likelihood.png" style="display:block; margin-left: auto; margin-right: auto;" title="Slide from Nishi's TestBash Home talk"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Slide from Nishi's TestBash Home talk&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But it does reflect the conversations &lt;a href="http://quality-intelligence.com/documents/DesignBehindthePlan.pdf"&gt;Fiona Charles's test strategy workshop&lt;/a&gt; encouraged me to spark on my team. We do talk about "Yes, this would be a problem, but customers can use this work-around." Or "Yes, we could dive in and investigate whether than could ever happen, but is that more important than picking up the next story?" Being able to identify risks and have thoughtful conversations about their threat to stakeholders allows us to make informed decisions about how we should be spending our time. In testing, we don't always want the most information, we want to discover the best information about the product as efficiently as we can. &lt;/p&gt;
&lt;h4&gt;Examples from my current project&lt;/h4&gt;
&lt;h5&gt;Cross-browser testing&lt;/h5&gt;
&lt;p&gt;We were preparing our web application for a big marketing presentation. The presenter had Firefox as the default browser on their PC. We had a script of the actions they'd perform on stage, and which pages the audience would see. I happened to find bugs on pages we weren't showing, or in the way the scroll bars behaved in Chrome rather than Firefox on my Mac. &lt;/p&gt;
&lt;p&gt;I did not add these issues as bugs in our tracking system, or dig into them further. I knew that they did not pose a risk for the presentation, and a new design would be coming along before customers would potentially use those pages in Chrome on a Mac.s &lt;/p&gt;
&lt;h5&gt;The pipeline&lt;/h5&gt;
&lt;p&gt;We have a pipeline. It runs the tests we've automated at the API and the browser levels against the build in our test environment. I hoped it would inspire the team to think about what the next step could be: getting the tests to run against before merging into our main line, setting up an environment where we're not dependent on the (shared) test environment, looking at the results to see where our application or tests need to change. &lt;/p&gt;
&lt;p&gt;But we don't look at the results. We don't have alerts, we don't open the page during standup, we don't use them as a reference when we're debugging, we don't have a habit of looking at the results. If we do happen to look at the results, we don't take action on it. Building the stability of our feedback loop is not seen as high-priority a task as building new features. &lt;/p&gt;
&lt;p&gt;We don't need to run this pipeline. It's using up AWS resources. Looking at the long line of red X's on the results page only provides alert fatigue. We would be better served by not running these tests. &lt;/p&gt;
&lt;h5&gt;Minimum viable deadline&lt;/h5&gt;
&lt;p&gt;We promised to deliver a feature to a dependent team by a sadline. (A sadline is a deadline without consequences.) In the week before the sadline, three stories were left. On the first story, I found a mistake the developer declared "superficial" when he was lamenting our lack of &lt;a href="https://katrinatester.blogspot.com/2016/12/the-testing-pendulum-finding-balance-in.html"&gt;deep testing&lt;/a&gt;. He decided to review the automated tests I'd written for the second story. He found a couple of use-cases that would require a very particular set of circumstances to occur. I wanted to encourage the behavior of reviewing the tests and thinking about what they're doing more deeply, so I spent the last hour and a half before a holiday weekend automating these two cases. &lt;/p&gt;
&lt;p&gt;I'd drafted some basic automated tests for the third story, but the last feature went relatively unexplored. I should have used my scant time to test the third story more thoroughly instead. The complicated tests for the second story could have waited until next week. While we would be curious about the results, it would not have stopped our delivery of the feature. I should not have written them. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/far-side-tree-falling.jpg" style="display:block; margin-left: auto; margin-right: auto;" title="Far Side cartoon"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Far Side cartoon&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;You may be scared to say no to testing things that don't matter, where the performance will not reveal any risks or cause any follow-up actions to take place. It may be tempting to spend a bunch of time testing all the things you can think of, and only reporting on the tests that yield meaningful results. &lt;/p&gt;
&lt;p&gt;But life is not about keeping busy. Make your time at work meaningful by executing meaningful work and declining to do things that aren't important right now.&lt;/p&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>testbash</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</guid><pubDate>Sat, 23 May 2020 22:00:00 GMT</pubDate></item></channel></rss>