<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elizabeth Zagroba: Software Tester (Posts about exploratory-testing)</title><link>https://elizabethzagroba.com/</link><description></description><atom:link href="https://elizabethzagroba.com/categories/exploratory-testing.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Â© 2021 &lt;a href="mailto:me@elizabethzagroba.com"&gt;Elizabeth Zagroba&lt;/a&gt; Mozilla Public License 2.0</copyright><lastBuildDate>Sat, 09 Jan 2021 15:35:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The Flow of Two Exploratory Testing Sessions</title><link>https://elizabethzagroba.com/posts/2020/2020-12-26_flow_of_two_exploratory_testing_sessions/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;The &lt;a href="https://exploratorytesting.org/"&gt;Exploratory Testing Peer Conference&lt;/a&gt; &lt;a href="https://twitter.com/search?q=%23et19&amp;amp;src=typed_query"&gt;#ET19&lt;/a&gt; followed the 2019 Valencia edition of the European Testing Conference. I said something that I pretty immediately forgot in the upheaval when I returned to the office, but it really stuck with &lt;a href="https://twitter.com/charrett"&gt;Anne-Marie Charrett&lt;/a&gt;. I appreciate both that she remembered this at all, and continues to insist that I'm the one who had this stroke of genius when she really brought it to life. Here's the idea: &lt;/p&gt;
&lt;p&gt;It's straightforward to follow one thread or idea through an exploratory testing session. It's not straightforward to decide which path to take, feed information from one path back into another, recognize that there are different paths, or bring others along for this journey. &lt;/p&gt;
&lt;h4&gt;The Ensemble at Work&lt;/h4&gt;
&lt;p&gt;We have a weekly ensemble testing session at my workplace. For an hour and a half, testers from different teams working in the same tech stack come together to share knowledge and build testing skills. In a recent ensemble testing session, a tester on one team brought a ticket they'd been avoiding tackling on their own. They knew they didn't know how to test the fix. But they felt like they'd talked about it enough as a team that they should have understood what to do already. &lt;/p&gt;
&lt;p&gt;We read through the story with the group of testers. We determined that a static code analysis security scan had discovered vulnerabilities in a couple of libraries. The developers had fixed the issue by removing the libraries. It was our mission to make sure those libraries were removed. &lt;/p&gt;
&lt;p&gt;Immediately a plan came to my mind:
1. Map out what kinds of pages there were, assuming that different pages of the same type would be likely to load the same libraries: list view, detail view, landing page, etc. 
1. Look at one of each of those pages with the Network tab open in the developer tools.&lt;/p&gt;
&lt;p&gt;In a quick spirt of excitement, I dumped this idea on the group without figuring out if people knew what either of these things meant. (It turns out, not everyone did.) But everyone seemed to understand that there was somewhere in the developer tools where we could tell which libraries were loaded, so we started there. Exploring in a group is not about getting everyone to follow my idea immediately (&lt;a href="https://www.showingupforracialjustice.org/white-supremacy-culture-characteristics.html"&gt;or ever&lt;/a&gt;), it's about making sure everyone is on board and understands what's going on.&lt;/p&gt;
&lt;p&gt;Proving the absence of a thing is harder than proving the presence of something, so we spent a bit of time looking through the Console and Storage tabs, as well as reloading the page with the Network tab opened, to figure out what appeared where. That helped everyone remember or discover that we didn't need to reload the page if the Network tab was open before the page loaded. This sped us up for the rest of the session.&lt;/p&gt;
&lt;p&gt;Next, we looked at a couple of similar-looking list pages. We searched for the libraries in the Network tab. They weren't there. Now that we'd seen a couple of examples, I decided it was the right time to bring up my original idea of grouping pages by type. (Going from the abstract to the concrete doesn't work for everybody, so sometimes going from the concrete to the abstract works better.) I asked "These last two pages both looked like list pages, what other kinds of pages are there? Can we list them? Should we look at a detail page?" This comment blew the mind of the tester who brought this ticket. They'd been testing this product for two years and had never organized the product this way in their brain. It may not have occurred to them that a product &lt;em&gt;could&lt;/em&gt; be organized in different ways in their thoughts depending on the circumstance. We got as far as listing the concrete pages we'd checked, but not as far as identifying all the types in the abstract before the energy in the ensemble moved on.&lt;/p&gt;
&lt;p&gt;We looked at a detail page. We looked at a settings page. Then one of the testers who's been looking at a lot of front-end Javascript noticed two things: both the URLs we were searching for had &lt;code&gt;ajax&lt;/code&gt; in them, so we only needed to search for one thing on each page we opened. And second, they knew that ajax was used to make changes to pages that had already loaded, so they asked "what kinds of pages change after they're loaded?" In this particular application, it was mostly forms in pop-up windows, so we concentrated our efforts there for the rest of the session. &lt;/p&gt;
&lt;p&gt;The whole session took about an hour and a half. A tester that came in scared and confused left empowered, with information to bring to their developers, and  a plan for how to execute the rest of the testing. Here's one way of looking at our exploratory testing session:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://elizabethzagroba.com/images/posts/2020/work-ensemble.png"&gt;&lt;img src="https://elizabethzagroba.com/images/posts/2020/work-ensemble.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At every stage, we absorbed a lesson as a group, and used it as our new superpower to make our testing better for the next bit. There were other paths we could have pursued, but many of these weren't consciously mentioned or acknowledged during the session.&lt;/p&gt;
&lt;h4&gt;The Ensemble at the Conference&lt;/h4&gt;
&lt;p&gt;I facilitated a couple of ensemble sessions with groups at Agile Testing Days. Our first emsemble had a couple people drop out, so it ended up being one tester, one developer, and me. We were looking at a &lt;a href="https://eviltester.github.io/TestingApp/apps/7charval/simple7charvalidation.htm"&gt;very straightforward application&lt;/a&gt; from Alan Richardson where you can decide whether a string contains 7 characters and is valid (in the set A-Z, a-z, 0-9, and * ). A few different times the developer and I asked if we should look at the source code. Rather than trying to interrogate the application based on the behavior from different inputs (black box testing), we wanted to go to the source (white box testing). &lt;/p&gt;
&lt;p&gt;But we never did. We kept trying different inputs, getting increasingly creative with order, special characters, Unicode characters, other languages as we progressed. But we never chose a different path. Even as I tried to encourage us to take notes so we wouldn't try the same things we'd already tried, we didn't. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://elizabethzagroba.com/images/posts/2020/atd-ensemble.png"&gt;&lt;img src="https://elizabethzagroba.com/images/posts/2020/atd-ensemble.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We did manage to find a good resource for copying and pasting unicode characters, but we didn't learn how to explore the application more efficiently, or take what we learned earlier in the session to apply it to the rest of the session.&lt;/p&gt;
&lt;h4&gt;The Power of Exploratory Testing&lt;/h4&gt;
&lt;p&gt;Brute force will get you somewhere. Trying enough different inputs, or different pages, and you'll gather more information about how the application works. But the power of exploratory testing comes from learning from your earlier results. It's realizing there are different ways to go, different paths to follow, jumping on one of those while it serves you, and making sure everyone else is along for the ride. &lt;/p&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>teaching</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-12-26_flow_of_two_exploratory_testing_sessions/</guid><pubDate>Fri, 25 Dec 2020 23:00:00 GMT</pubDate></item><item><title>Finding relevant search results</title><link>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;In his crafting time, one of our developers decided to fine-tune our search results. He added relevancy scoring to give weights to different text fields. It was my job to determine if the right results were turning up at the top of the list of search results. So I had to ponder: what made a search result relevant? &lt;/p&gt;
&lt;p&gt;First, I realized that feedback from our users is the best way to answer this question. Anything I could do to get this feature out into production, where we'd get real data about what people searched for, would be more valuable that brainstorming test cases for this feature. I set myself a timebox of two and a half hours, the rest of the afternoon on a day in a week filled with competing priorities. We'd agreed as a team ahead of time that I could determine the testing approach, and our product owner would decide what was or wasn't worth fixing before this feature went out. &lt;/p&gt;
&lt;p&gt;I saved the first 45 minutes of my timebox to research what people had to say about search relevancy. Surely I was not the first person contemplating this problem. Over on the Ministry of Testing Club forum, I found what seemed to be a promising title of a post. &lt;a href="https://club.ministryoftesting.com/t/how-would-you-test-a-search-api/28027"&gt;It turned out a past Elizabeth from a year ago wrote it, and nobody had answered it satisfactorily in the intervening time&lt;/a&gt;. ð¤¦ââï¸&lt;/p&gt;
&lt;p&gt;After Duck-Duck-Go-ing some actual answers, I found a couple of resources from websites I've trusted and found fruitful in the past: &lt;a href="https://alistapart.com/article/testing-search-for-relevancy-and-precision/"&gt;A List Apart&lt;/a&gt; and &lt;a href="https://www.philosophe.com/archived_content/search_topics/search_tests.html"&gt;philosophe.com&lt;/a&gt;. A List Apart suggested honing in on a specific intent, searching for the queries users seek most frequently, and seeing how far those items fall from the top in the search results. The philosophe guidance about testing search gave me something deeper to consider: users shouldn't have to ponder the reasoning behind the search results. That was enough for me to develop some test cases. &lt;/p&gt;
&lt;p&gt;As I searched and adjusted the weights of various fields, plenty of normal things happened: setting the relevancy values to zero meant the result wasn't listed, multiple instances of the same word counted more than a single instance, and giving fields stronger weights caused their results to move up in the rankings. But as a bug magnet, I uncovered things that were both interesting to discover and outside the original scope of the story. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/log-of-zero.png" style="display:block; margin-left: auto; margin-right: auto;" title="Log of zero is negative infinity"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Log of zero is negative infinity&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Bugs&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I opted to edit data that was already in our test environment rather than setting up completely new data. In doing so, I discovered a couple description fields that were missing an indexing update in the search results when edited through the UI. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I tried to use the default values for everything to see "normal" results. One field was going to add a value to the relevancy rating, so zero seemed like it should be the default option. Unfortunately a couple of the options for the weighting feature transformed the value using the &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;ln&lt;/code&gt; (natural log) functions, which are undefined at zero. All my search results disappeared. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I looked at the data that was already in the database, and used a search term that showed up a lot already. It turned up nothing. I searched for part of the word. That turned up all the results I was expecting. I realized the search term was indexed separately because of the characters we used to break the word apart. Imagine having a bunch of &lt;code&gt;sun-bleached&lt;/code&gt; items, but you can only find them if you search for &lt;code&gt;sun&lt;/code&gt; or &lt;code&gt;bleached&lt;/code&gt;, not &lt;code&gt;sun-bleached&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bug 1 the developer agreed was a bug, and we fixed as part of the story. Bug 2 was "working as expected" from a developer point-of-view, but seemed a little weird to the product owner. We meant to look into as part of the story and decide if we should eliminate the log functions as options entirely, but other priorities came crashing down upon before we could. It's out on production to the handful of internal users with access to the relevancy tuning. Bug 3 we added to the backlog, and I hope someday a swarm of user complaints make it a priority for us. &lt;/p&gt;
&lt;h4&gt;Morals&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Users know best.&lt;/li&gt;
&lt;li&gt;Someone else on the internet has had your problem before. &lt;/li&gt;
&lt;li&gt;Report information that matters.&lt;/li&gt;
&lt;li&gt;When the risk is low, let it go. &lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>search</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-09-21_finding_relevant_search_results/</guid><pubDate>Sun, 20 Sep 2020 22:00:00 GMT</pubDate></item><item><title>If a test falls in a forest...</title><link>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p&gt;The saying goes "If a tree falls in a forest and no one is around to hear it, does it make a sound?" I have similar question that shapes the way I think about software testing: If a test is performed but no one takes action on the results, should I have performed it? I think not. &lt;/p&gt;
&lt;p&gt;If the answer to "Who cares?" is "No one," don't perform that test. If you're not going to take action on the results of your testing in the coming hours, days, or weeks, don't perform that test. The world around you will change in the meantime, and the old results will not be as valuable.&lt;/p&gt;
&lt;p&gt;One of the &lt;a href="https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"&gt;12 Agile Principles&lt;/a&gt; is simplicity, or maximizing the work not done. Testing on an agile team provides information to help decide what work should picked up in the coming iteration(s). But without meaningful collaboration or feedback, testing is a pile of work for no reason. Work is not meant to produce waste. Save your time and your sanity by thoughtfully analyzing what should not be done, and coming to an agreement with your team about it.&lt;/p&gt;
&lt;p&gt;My team gets scared about the quality of our product and skeptical about how I'm using my time when I describe what I'm not testing, or which automated tests I'm not going to run. "But isn't testing your job?" says the look on their faces. "But then what are you going to do?" is what they manage to say. Rather than capitulating for appearances, to just "look busy," I take this as a challenge to make my exploratory testing and other work I'm doing for the team more visible. &lt;/p&gt;
&lt;h4&gt;Risk-based testing&lt;/h4&gt;
&lt;p&gt;In her &lt;a href="https://www.ministryoftesting.com/dojo/series/testbash-home/lessons/reverse-engineer-your-way-to-adopting-a-risk-based-testing-approach-nishi-grover-garg"&gt;TestBash Home talk&lt;/a&gt;, &lt;a href="https://twitter.com/testwithnishi"&gt;Nishi Grover Garg&lt;/a&gt; asked us to think about estimating impact and likelihood (with possible home intruders as an example). I'd have trouble pinning down our no-estimates team on concrete numbers for undesireable software behavior. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/nishi-impact-likelihood.png" style="display:block; margin-left: auto; margin-right: auto;" title="Slide from Nishi's TestBash Home talk"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Slide from Nishi's TestBash Home talk&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But it does reflect the conversations &lt;a href="http://quality-intelligence.com/documents/DesignBehindthePlan.pdf"&gt;Fiona Charles's test strategy workshop&lt;/a&gt; encouraged me to spark on my team. We do talk about "Yes, this would be a problem, but customers can use this work-around." Or "Yes, we could dive in and investigate whether than could ever happen, but is that more important than picking up the next story?" Being able to identify risks and have thoughtful conversations about their threat to stakeholders allows us to make informed decisions about how we should be spending our time. In testing, we don't always want the most information, we want to discover the best information about the product as efficiently as we can. &lt;/p&gt;
&lt;h4&gt;Examples from my current project&lt;/h4&gt;
&lt;h5&gt;Cross-browser testing&lt;/h5&gt;
&lt;p&gt;We were preparing our web application for a big marketing presentation. The presenter had Firefox as the default browser on their PC. We had a script of the actions they'd perform on stage, and which pages the audience would see. I happened to find bugs on pages we weren't showing, or in the way the scroll bars behaved in Chrome rather than Firefox on my Mac. &lt;/p&gt;
&lt;p&gt;I did not add these issues as bugs in our tracking system, or dig into them further. I knew that they did not pose a risk for the presentation, and a new design would be coming along before customers would potentially use those pages in Chrome on a Mac.s &lt;/p&gt;
&lt;h5&gt;The pipeline&lt;/h5&gt;
&lt;p&gt;We have a pipeline. It runs the tests we've automated at the API and the browser levels against the build in our test environment. I hoped it would inspire the team to think about what the next step could be: getting the tests to run against before merging into our main line, setting up an environment where we're not dependent on the (shared) test environment, looking at the results to see where our application or tests need to change. &lt;/p&gt;
&lt;p&gt;But we don't look at the results. We don't have alerts, we don't open the page during standup, we don't use them as a reference when we're debugging, we don't have a habit of looking at the results. If we do happen to look at the results, we don't take action on it. Building the stability of our feedback loop is not seen as high-priority a task as building new features. &lt;/p&gt;
&lt;p&gt;We don't need to run this pipeline. It's using up AWS resources. Looking at the long line of red X's on the results page only provides alert fatigue. We would be better served by not running these tests. &lt;/p&gt;
&lt;h5&gt;Minimum viable deadline&lt;/h5&gt;
&lt;p&gt;We promised to deliver a feature to a dependent team by a sadline. (A sadline is a deadline without consequences.) In the week before the sadline, three stories were left. On the first story, I found a mistake the developer declared "superficial" when he was lamenting our lack of &lt;a href="https://katrinatester.blogspot.com/2016/12/the-testing-pendulum-finding-balance-in.html"&gt;deep testing&lt;/a&gt;. He decided to review the automated tests I'd written for the second story. He found a couple of use-cases that would require a very particular set of circumstances to occur. I wanted to encourage the behavior of reviewing the tests and thinking about what they're doing more deeply, so I spent the last hour and a half before a holiday weekend automating these two cases. &lt;/p&gt;
&lt;p&gt;I'd drafted some basic automated tests for the third story, but the last feature went relatively unexplored. I should have used my scant time to test the third story more thoroughly instead. The complicated tests for the second story could have waited until next week. While we would be curious about the results, it would not have stopped our delivery of the feature. I should not have written them. &lt;/p&gt;
&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2020/far-side-tree-falling.jpg" style="display:block; margin-left: auto; margin-right: auto;" title="Far Side cartoon"&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;Far Side cartoon&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;You may be scared to say no to testing things that don't matter, where the performance will not reveal any risks or cause any follow-up actions to take place. It may be tempting to spend a bunch of time testing all the things you can think of, and only reporting on the tests that yield meaningful results. &lt;/p&gt;
&lt;p&gt;But life is not about keeping busy. Make your time at work meaningful by executing meaningful work and declining to do things that aren't important right now.&lt;/p&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>risk-based-testing</category><category>testbash</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2020/2020-05-24_if_a_test_falls_in_a_forest/</guid><pubDate>Sat, 23 May 2020 22:00:00 GMT</pubDate></item><item><title>Exploratory Testing with the Chrome Network Tab</title><link>https://elizabethzagroba.com/posts/2019/2019-03-29_exploratory-testing-with-the-chrome-network-tab/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p name="6c32" id="6c32" class="graf graf--p graf-after--h3"&gt;I needed to the loading time of a login over a slow network. The internet connection I had was too fast to see all the visual behavior and the backend redirects happening during the process. I opened the Network tab in the Chrome developer tools and switched the throttling option to &lt;code class="markup--code markup--p-code"&gt;Slow 3G&lt;/code&gt;. (A yellow triangular yield symbol appeared next to the Network tab to remind me that Iâd throttled my network.) Running over &lt;code class="markup--code markup--p-code"&gt;Slow 3G&lt;/code&gt; allowed me to see what someone trying access the site from a phone or tablet might experience.&lt;/p&gt;

&lt;p name="cd6d" id="cd6d" class="graf graf--p graf-after--p"&gt;&lt;em class="markup--em markup--p-em"&gt;The screenshots below are from the login on hackdesign.org, a program I highly recommend for getting up-to-speed on user experience design.&lt;/em&gt;&lt;/p&gt;

&lt;p name="29ea" id="29ea" class="graf graf--p graf-after--p"&gt;With the Network tab open, I could do a few things:&lt;/p&gt;

&lt;ol class="postList"&gt;&lt;li name="8374" id="8374" class="graf graf--li graf-after--p"&gt;I could see what API calls were being made. I looked at the bottom of the &lt;code class="markup--code markup--li-code"&gt;Name&lt;/code&gt; column to see how many calls there were overall, and sorted it to discover if we were retrieving things from the server that I expected to be cached. I clicked the &lt;code class="markup--code markup--li-code"&gt;Preserve log&lt;/code&gt; checkbox before I started so I could see what happened even after I went to another page.&lt;/li&gt;&lt;li name="ef06" id="ef06" class="graf graf--li graf-after--li"&gt;I could see which calls were redirects. The &lt;code class="markup--code markup--li-code"&gt;Status&lt;/code&gt;column had numbers in the 300 range for redirects. I love&lt;a href="https://httpstatuses.com/" data-href="https://httpstatuses.com/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"&gt; httpstatuses.com&lt;/a&gt; for what each one means more precisely. Redirects might indicate something could be optimized.&lt;/li&gt;&lt;li name="6d41" id="6d41" class="graf graf--li graf-after--li"&gt;I could tell how much time each of those network calls took. The &lt;code class="markup--code markup--li-code"&gt;Time&lt;/code&gt; column allowed me to sort by milliseconds to find the call that took the longest.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;&lt;span class="img_container" style="display: inline-block;"&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2019/chrome-network-tab.png" style="display:block; margin-left: auto; margin-right: auto;" title="What I used in the Network tab of the Chrome developer tools. HackDesign.org login only took 6 seconds."&gt;&lt;span class="img_caption" style="display: block; text-align: center;"&gt;What I used in the Network tab of the Chrome developer tools. HackDesign.org login only took 6 seconds.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p name="de62" id="de62" class="graf graf--p graf-after--figure"&gt;I discovered logging in and logging out took about 20 seconds on the test environment over Slow 3G. (This time appeared in the &lt;code class="markup--code markup--p-code"&gt;Load&lt;/code&gt; text in red at the bottom of the Network tab.) Was this too slow? To answer this question, I needed an oracle.&lt;/p&gt;

&lt;p name="7704" id="7704" class="graf graf--p graf-after--p"&gt;I decided to compare the behavior on the test environment to our production environment. On production, login took 60 seconds! When I sorted the network calls by &lt;code class="markup--code markup--p-code"&gt;Time&lt;/code&gt;, I could see that the bulk of loading time was spent retrieving messages to display on the logged-in page. Both 20 and 60 seconds for a login seemed unacceptably slow to me, so I took it to my team.&lt;/p&gt;

&lt;p name="d153" id="d153" class="graf graf--p graf-after--p"&gt;My team agreed that this behavior was bad. Unfortunately, we decided to prioritize users on fast networks over users on slow ones, and changing this behavior wasnât a priority for our release.&lt;/p&gt;

&lt;p name="f224" id="f224" class="graf graf--p graf-after--p"&gt;When I sorted the network calls by &lt;code class="markup--code markup--p-code"&gt;Name&lt;/code&gt;, I found some unexpected URLs I did not expect to be involved during a login. I tested a bit more around the feature in different places, found a bug in the behavior, and asked around a few different teams before I got the bug reported to the correct team.&lt;/p&gt;

&lt;h5 name="d83e" id="d83e" class="graf graf--h4 graf-after--p"&gt;Moral of theÂ story&lt;/h5&gt;

&lt;p name="715e" id="715e" class="graf graf--p graf-after--h4 graf--trailing"&gt;You have a powerful web performance testing tool at your disposal. Give it a try and see what you find.&lt;/p&gt;

&lt;p&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Originally published on &lt;a href="https://medium.com/@ezagroba/exploratory-testing-with-the-chrome-network-tab-f093e1b3d725"&gt;Medium&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2019/2019-03-29_exploratory-testing-with-the-chrome-network-tab/</guid><pubDate>Thu, 28 Mar 2019 23:00:00 GMT</pubDate></item><item><title>Have I Tried Enough WeirdÂ Stuff?</title><link>https://elizabethzagroba.com/posts/2019/2019-01-25_have-i-tried-enough-weird-stuff/</link><dc:creator>Elizabeth Zagroba</dc:creator><description>&lt;div&gt;&lt;p name="fd7f" id="fd7f" class="graf graf--p graf-after--h3"&gt;I was testing a piece of software that collected a personâs addresses for shipping within the United States. My developer had tried zip codes in the direct vicinity of our office in Manhattan, which all started with 1. I tried the zip codes for my hometown in New Jersey and the college I attended in Maine, both of which started with 0. Together we determined that the zip codes (and other address fields) needed to be stored differently so leading zeros would not be cut off. But it got me thinking: what other things might occur that were outside the direct experience of me and my developer?&lt;/p&gt;

&lt;p name="83bc" id="83bc" class="graf graf--p graf-after--p"&gt;So I asked the internet.&lt;/p&gt;

&lt;p name="cf4f" id="cf4f" class="graf graf--p graf-after--p"&gt;Thatâs when I first came across &lt;a href="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/" data-href="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;Falsehoods programmers believe about addresses&lt;/a&gt;. We were constrained to collecting American shipping addresses, so things like âare the odd street numbers all on the same side?â werenât our concern. But plenty of them were. Was our form going to allow people whose shipping address was any of these?&lt;/p&gt;

&lt;ul class="postList"&gt;&lt;li name="a522" id="a522" class="graf graf--li graf-after--p"&gt;a post office box&lt;/li&gt;&lt;li name="cf1b" id="cf1b" class="graf graf--li graf-after--li"&gt;outside one of the fifty states (Washington D.C., Puerto Rico, Guam, etc.)&lt;/li&gt;&lt;li name="766c" id="766c" class="graf graf--li graf-after--li"&gt;on an American military base&lt;/li&gt;&lt;li name="dda7" id="dda7" class="graf graf--li graf-after--li"&gt;a fractional number&lt;/li&gt;&lt;/ul&gt;

&lt;p name="57b2" id="57b2" class="graf graf--p graf-after--li"&gt;As I tested inputs on other applications, I kept wondering if I was only thinking of things I already knew about, or if the problem space was bigger than I could conceive. Iâve come across a few lists that I love to review with my developers before they start building an input field (or an API parameter) so we can agree on what kind of validation weâre going to do.&lt;/p&gt;

&lt;p name="bdeb" id="bdeb" class="graf graf--p graf-after--p"&gt;&lt;a href="http://testobsessed.com/wp-content/uploads/2011/04/testheuristicscheatsheetv1.pdf" data-href="http://testobsessed.com/wp-content/uploads/2011/04/testheuristicscheatsheetv1.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;The Test Heuristics Cheat Sheet&lt;/a&gt; provides a great jumping-off point specific inputs for text fields on the first page and different ways to try inputting them on the second page.&lt;/p&gt;

&lt;p name="97d8" id="97d8" class="graf graf--p graf-after--p"&gt;&lt;a href="https://github.com/minimaxir/big-list-of-naughty-strings" data-href="https://github.com/minimaxir/big-list-of-naughty-strings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;The Big List of Naughty Strings&lt;/a&gt; collects different kinds of characters (languages with non-Roman characters, emojis, Javascript that might trigger script injection, etc.) in one place so I donât have to search for each of these cases individually. &lt;a href="https://github.com/minimaxir/big-list-of-naughty-strings/blob/master/blns.txt" data-href="https://github.com/minimaxir/big-list-of-naughty-strings/blob/master/blns.txt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;I usually copy-paste the ones weâve agreed we want to support from here&lt;/a&gt;. [Note: I recommend bookmarking this repository so youâre not accidentally getting NSFW results after searching ânaughty strings.â]&lt;/p&gt;

&lt;p name="0d24" id="0d24" class="graf graf--p graf-after--p"&gt;Searching for âFalsehoods programmers believe about {input type}â is my go-to for more specific types of inputs. &lt;a href="https://github.com/kdeldycke/awesome-falsehood" data-href="https://github.com/kdeldycke/awesome-falsehood" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;Thereâs a list of a bunch of them&lt;/a&gt;, but these are some of my favorites:&lt;/p&gt;

&lt;ul class="postList"&gt;&lt;li name="20c5" id="20c5" class="graf graf--li graf-after--p"&gt;&lt;a href="http://falsehoodsabouttime.com" data-href="http://falsehoodsabouttime.com" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"&gt;Time&lt;/a&gt;&lt;/li&gt;&lt;li name="f7e6" id="f7e6" class="graf graf--li graf-after--li"&gt;&lt;a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" data-href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"&gt;Names&lt;/a&gt;&lt;/li&gt;&lt;li name="b82f" id="b82f" class="graf graf--li graf-after--li"&gt;&lt;a href="https://wiesmann.codiferes.net/wordpress/?p=15187" data-href="https://wiesmann.codiferes.net/wordpress/?p=15187" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"&gt;Geography&lt;/a&gt;&lt;/li&gt;&lt;li name="aed1" id="aed1" class="graf graf--li graf-after--li"&gt;&lt;a href="https://haacked.com/archive/2007/08/21/i-knew-how-to-validate-an-email-address-until-i.aspx/" data-href="https://haacked.com/archive/2007/08/21/i-knew-how-to-validate-an-email-address-until-i.aspx/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"&gt;Emails&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p name="8e3e" id="8e3e" class="graf graf--p graf-after--li"&gt;I encourage you to keep asking âhave I tried enough weird stuff?â and deciding together with your developers what constitutes âweird.â&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://elizabethzagroba.com/images/posts/2019/arrows.jpeg"&gt;&lt;/p&gt;
&lt;section name="7d8d" class="section section--body section--last"&gt;&lt;div class="section-divider"&gt;&lt;hr class="section-divider"&gt;&lt;/div&gt;&lt;div class="section-content"&gt;&lt;div class="section-inner sectionLayout--insetColumn"&gt;&lt;p name="1dd4" id="1dd4" class="graf graf--p graf--leading graf--trailing"&gt;&lt;em class="markup--em markup--p-em"&gt;Thanks to Trish Khoo and Anne-Marie Charrett for the impetus to publish this, and Joep Schuurkes for pointing out that my headline falls under &lt;/em&gt;&lt;a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" data-href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"&gt;&lt;em class="markup--em markup--p-em"&gt;Betteridgeâs law&lt;/em&gt;&lt;/a&gt;&lt;em class="markup--em markup--p-em"&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;



&lt;p&gt;&lt;em&gt;Originally published on &lt;a href="https://medium.com/@ezagroba/have-i-tried-enough-weird-stuff-7ed4105ae994"&gt;Medium&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>exploratory-testing</category><category>testing</category><guid>https://elizabethzagroba.com/posts/2019/2019-01-25_have-i-tried-enough-weird-stuff/</guid><pubDate>Thu, 24 Jan 2019 23:00:00 GMT</pubDate></item></channel></rss>